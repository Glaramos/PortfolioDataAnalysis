<!DOCTYPE html>
<html lang="es">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ETL | Portafolio</title>

  <!-- Fuentes elegantes -->
  <link
    href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;600&family=Open+Sans:wght@300;400;600&display=swap"
    rel="stylesheet" />

  <!-- Bootstrap -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet" />

  <!-- Iconos -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css" rel="stylesheet" />

  <!-- Estilos personalizados -->
  <link rel="stylesheet" href="assets/css/style.css" />
</head>

<body>
  <!-- Navbar -->
  <nav class="navbar navbar-expand-lg fixed-top">
    <div class="container">
      <a class="navbar-brand fw-semibold" href="index.html">Gladys Ramos</a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse justify-content-end" id="navbarNav">
        <ul class="navbar-nav">
          <li class="nav-item"><a class="nav-link" href="index.html"><i class="bi bi-house-door"></i> Home</a></li>
          <li class="nav-item"><a class="nav-link" href="formacion.html">Education & Training</a></li>
          <li class="nav-item"><a class="nav-link active" href="etl.html">ETL</a></li>
          <li class="nav-item"><a class="nav-link" href="sql.html">SQL</a></li>
          <li class="nav-item"><a class="nav-link" href="dashboard.html">Dashboard</a></li>
        </ul>
      </div>
    </div>
  </nav>

  <!-- Contenido principal -->
  <div class="container-main mt-5 pt-5">

    <canvas id="dataCanvas"></canvas>

    <div class="section formation">

      <h2>ETL Skills & Tools</h2>

      <p>
        I perform <strong>ETL (Extract, Transform, Load)</strong> processes using
        <strong style="color:#0073e6;">Python</strong>,
        <strong style="color:#0073e6;">SQL</strong>,
        <strong style="color:#0073e6;">Pandas</strong>, and
        <strong style="color:#0073e6;">Excel</strong> to extract, clean, and transform data from various sources.
        In addition to loading and visualizing data in Power BI, I also develop
        <strong style="color:#e67300;">dashboards</strong> in Excel using pivot tables, charts, and slicers,
        enabling fast and flexible analysis.
      </p>

      <p>
        This workflow allows me to automate processes, ensure data quality, and create efficient and well-structured
        analytical solutions that support decision-making.
      </p>



      <!-- Filtros -->
      <h5 class="mt-4">Filter by Topic</h5>

      <!-- Filtros -->
      <div class="mb-4 d-flex flex-wrap gap-2">
        <button class="btn btn-outline-secondary tag-filter-btn active" data-tag="all">Todos</button>
        <button class="btn btn-outline-secondary tag-filter-btn" data-tag="YM">Yerba mate</button>
        <button class="btn btn-outline-secondary tag-filter-btn" data-tag="SM">Supermarket</button>
        <button class="btn btn-outline-secondary tag-filter-btn" data-tag="AG">Agronomic</button>
        <button class="btn btn-outline-secondary tag-filter-btn" data-tag="LG">Logitc</button>
      </div>

      <!-- Sección de ETL -->
      <div class="section etl">
        <h2>ETL Projects</h2>

        <div class="project-card formation-card" data-tags="YM">
          <header>
            <h1>History of the Data</h1>
          </header>

          <main>
            <section>
              <h2>Origin of the Data</h2>
              <p>
                The “Yerba Mate” project was born from the need to consolidate, clean, and prepare for analysis a real
                dataset of yerba mate production between the years 2022 and 2024.
                It includes information about different farms, geographic zones, cultivated hectares, harvested kilos,
                type of crop, rainfall, operational costs, and metrics such as yield per hectare and cost per kilogram.
                It covers yerba mate production managed by a company in the agro-industrial sector.
                The organization recorded daily operational information through two main sources:
                The internal system database, built on MySQL, which stored tables related to:

                harvest records,

                agronomic data per farm,

                operational cost history,

                environmental information associated with each plot.

                Excel files generated by field personnel, used to document:

                daily activity planning,

                actual harvested volumes,

                harvesting times,

                fertilization conditions,

                manual corrections to operational data.

                Since each source followed its own recording criteria—with different formats, inconsistent names,
                spelling
                variations, and differences in units—the company lacked a unified dataset that could be directly used
                for
                analysis, reporting, or data-driven decision-making.
              </p>

              <p>
                Although this dataset was functional, it presented numerous common issues found in agro-industrial data:
                missing values, misspelled text, incomplete columns, inconsistent records, and typing errors that
                prevented reliable analysis. For this reason, a complete cleaning and standardization process was
                developed using <strong>Python and Pandas</strong>.
              </p>

              <h3>Data Cleaning Operations</h3>

              <h4>Identification and Removal of Duplicate Records</h4>
              <p>
                During the first review, potentially duplicated rows were detected based on the combination of <em>Year,
                  Month, Farm, and Zone</em>. Some entries represented the same harvest event entered more than once.
                Only
                one valid version of each record was kept.
              </p>

              <h4>Detection and Imputation of Missing Values</h4>
              <p>
                Several critical columns contained missing values:
              <ul>
                <li><strong>Kilos_cosechados</strong> had completely empty records.</li>
                <li><strong>Costo_total</strong> showed a missing value in the year 2023.</li>
                <li><strong>Rendimiento_kg_ha</strong> had null values that could be calculated from other columns.</li>
              </ul>
              To ensure consistency, missing values were imputed according to their nature:
              <strong>means or medians</strong> for numerical data, and in the case of yield, it was recalculated as
              <code>Kilos_cosechados / Hectáreas</code>.
              </p>

              <h4>Correction of Formatting Issues and Label Standardization</h4>
              <p>
                This dataset contained multiple textual inconsistencies:
              <ul>
                <li>Farms written as <em>"Finc@ @"</em>, <em>"Finca cx"</em>, or with missing spaces.</li>
                <li>Zones like <em>"Centr0"</em> or <em>"No"</em> instead of “Centro” and “Norte”.</li>
                <li>Crop types such as <em>"C0nvenci0nal"</em>, <em>"Convencion@l"</em>, or <em>"Orgánic0"</em>.</li>
              </ul>
              Using normalization functions and replacement rules, all labels were standardized to:
              <strong>“Finca a / b / c”</strong>, <strong>“North / South / Center”</strong>,
              <strong>“Conventional / Organic”</strong>.
              </p>

              <h4>Correction of Out-of-Range Numerical Values</h4>
              <p>
                Strange values were identified in:
              <ul>
                <li><strong>Lluvia_mm</strong> with decimal values in the wrong format (“175.72”).</li>
                <li><strong>Costo_total</strong> empty or extremely low.</li>
                <li><strong>Tiempo_cosecha_dias</strong> missing or equal to zero.</li>
              </ul>
              These were corrected through:
              <strong>replacement with consistent values from the same period</strong>, or removal in critical cases.
              </p>

              <h4>Validation of the Dataset Structure</h4>
              <p>
                Once the errors were corrected, the following was verified:
              <ul>
                <li>That all columns had the correct type (numerical or categorical).</li>
                <li>That yields were coherent (Kilos / Hectares).</li>
                <li>That there were no out-of-range dates.</li>
                <li>That there were no logical inconsistencies (e.g., fertilized = “Yes” with atypically low costs).
                </li>
              </ul>
              </p>

              <h4>Exporting the Clean Version</h4>
              <p>
                Finally, a clean version of the dataset was generated and exported in CSV format. This version is now
                suitable for exploratory analysis (EDA), predictive models, or visualizations.
              </p>

              <h3>Exploratory Data Analysis (EDA)</h3>
              <p>
                With the cleaned data, key questions were explored:
              </p>
              <ul>
                <li>Which zone produces more kilos per hectare?</li>
                <li>Do organic crops yield lower production but incur higher costs?</li>
                <li>Is there a correlation between rainfall and production?</li>
                <li>Which farm achieves the most efficient cost per kg?</li>
              </ul>
              <p>
                Statistical summaries, comparative charts, and category-based analysis were used to answer these
                questions.
              </p>

              <h3>Conclusion</h3>
              <p>
                Thanks to this comprehensive process, the dataset went from being a collection of inconsistent records
                to
                a reliable source of information. The cleaning process enabled precise analysis and laid the foundation
                for visualization, dashboards, and data-driven decision-making within the yerba mate sector.
              </p>
              <p>
                The use of <strong>Python, Pandas, and Numpy</strong> made it possible to develop a solid and replicable
                pipeline—essential for any agricultural data analysis project.
              </p>

            </section>
          </main>


          <p></p>
          <hr>
          <p></p>

          <h2 class="project-title">Data Cleaning Yerba Mate</h2>

          <!-- Professional Summary -->
          <p>
            This project focuses on cleaning and standardizing a real dataset on yerba mate production, which contained
            common issues such as duplicates, missing values, numeric inconsistencies, and spelling variations in
            categorical fields.
            Using <strong>Python</strong> and <strong>Pandas</strong>, a complete data-cleaning pipeline was developed
            to:
          </p>
          <ul>
            <li>Identify and remove duplicate records</li>
            <li>Detect and impute missing values</li>
            <li>Correct formatting issues and standardize text labels</li>
            <li>Fix out-of-range numerical values</li>
            <li>Validate the dataset structure</li>
            <li>Export a clean version ready for further analysis</li>
          </ul>
          <p>
            The result is a reliable and consistent dataset prepared for <strong>exploratory data analysis
              (EDA)</strong>, modeling, or data visualization.
          </p>

          <p class="project-tech">Technology: Python, Pandas, Numpy</p>

          <div class="social-links">
            <a href="https://github.com/Glaramos" target="_blank"><i class="bi bi-github"></i></a>
            <a href="https://www.kaggle.com/code/gladysangelicaramos/data-cleaning-yerba-mate?scriptVersionId=270003987&cellId=3"
              target="_blank"><i class="bi bi-database"></i>view kaggle</a>
          </div>
          <!-- </div> -->

          <!-- Tarjeta 1 -->
          <!-- <div class="project-card"> -->

          <p></p>
          <hr>
          <p></p>

          <section id="eda-yerba-mate">
            <h2>Exploratory Data Analysis (EDA) - Yerba Mate</h2>

            <p>
              Exploratory Data Analysis (EDA) allows us to uncover patterns, relationships, and behaviors within a
              dataset.
              In this case, we will explore yerba mate production to answer questions such as:
            </p>

            <ul>
              <li>Which areas produce the most?</li>
              <li>How does the type of cultivation (organic or conventional) influence production?</li>
              <li>Is there a relationship between rainfall and production?</li>
              <li>What are the most efficient costs and yields?</li>
            </ul>

            <p class="project-tech">Technology: Python, Pandas</p>

            <div class="social-links">
              <a href="https://github.com/Glaramos" target="_blank"><i class="bi bi-github"></i></a>
              <a href="https://www.kaggle.com/code/gladysangelicaramos/an-lisis-exploratorio-de-datos-eda"
                target="_blank"><i class="bi bi-database"></i>view kaggle</a>

            </div>
        </div>

        <div class="project-card formation-card" data-tags="SM">
          <header>
            <h1>Data History</h1>
          </header>

          <main>
            <section>
              <h2>Data Origin</h2>
              <p>
                This project focuses on cleaning and standardizing a real dataset obtained from a supermarket’s sales
                receipts.
                The data presented common issues such as duplicates, missing values, inconsistencies in the products
                sold,
                and variations in payment methods.
              </p>
              <p>
                Using <strong>Python</strong> and <strong>Pandas</strong>, a data-cleaning pipeline was developed to:
              </p>
              <ul>
                <li>Remove duplicate sales records</li>
                <li>Impute missing values in total sales and products</li>
                <li>Normalize product categories</li>
                <li>Correct inconsistencies in payment methods</li>
                <li>Generate a clean dataset for further analysis</li>
              </ul>
              <p>
                The result is a reliable and consistent dataset, ready for sales analysis, product trend evaluation,
                and
                inventory optimization.
              </p>
            </section>

            <section>
              <h2>Data Issues</h2>

              <h3>Duplicates</h3>
              <p>
                Duplicate records were found due to a malfunction in the integration between the sales system and the
                billing system.
              </p>
              <p><strong>Solution:</strong> A process was implemented to identify and remove duplicates using SQL
                queries
                and <strong>Python</strong> scripts.</p>

              <h3>Missing Values</h3>
              <p>
                Some critical fields, such as the price of certain products and the total sale amount, were missing
                due
                to
                data entry errors.
              </p>
              <p><strong>Solution:</strong> Data imputation was applied using averages from previous sales, along with
                validation of missing fields through a <strong>Python</strong> script.</p>

              <h3>Inconsistencies in Payment Methods</h3>
              <p>
                Payment methods were not standardized, leading to inconsistent records between card payments, cash,
                and
                other methods.
              </p>
              <p><strong>Solution:</strong> Payment categories were normalized through an ETL process to uniformly
                classify
                payment methods.</p>
            </section>

            <section>
              <h2>Technology and Tools Used</h2>
              <p><strong>Database Engine:</strong> PostgreSQL, chosen for its performance in managing large volumes of
                transactional data.</p>
              <p><strong>Analysis Platform:</strong> <strong>Python</strong>, with libraries such as
                <strong>Pandas</strong>, <strong>NumPy</strong>, and <strong>Matplotlib</strong> for data cleaning and
                visualization.
              </p>
              <p><strong>Data Integration:</strong> An ETL pipeline developed in <strong>Python</strong> to extract,
                transform, and load data from the sales systems into PostgreSQL.</p>
            </section>

            <section>
              <h2>Impact on Decision-Making</h2>
              <p>
                Data cleaning has enabled supermarket analysts to generate more accurate reports on sales, top-selling
                products,
                and consumer trends. This information has supported more informed decisions regarding inventory and
                sales
                strategies.
              </p>
              <p>
                Additionally, data standardization has improved integration with other systems, optimizing inventory
                management and sales reporting.
              </p>
            </section>

            <hr>

          </main>

          <h2 class="project-title">Data Cleaning & Preparation – Supermarket Sales</h2>

          <!-- Professional Summary -->
          <p>
            This project showcases a complete data cleaning and standardization process applied to a supermarket sales
            dataset. The main tasks performed include:

            Using <strong>Python</strong> and <strong>Pandas</strong>, a complete data-cleaning pipeline was developed
            to:
          </p>
          <ul>
            <li>Removing duplicate records to ensure data integrity</li>
            <li>Handling missing values by assigning appropriate defaults and recalculating totals when necessary.
            </li>
            <li>Normalizing product categories, fixing typos, and unifying naming conventions.</li>
            <li>Standardizing payment methods, cleaning formats and grouping variations into consistent labels.</li>
            <li>Correcting general inconsistencies to improve dataset quality and support further analysis.</li>
          </ul>
          <p>

            The final result is a cleaned dataset ready for <strong>exploratory analysis, visualization, or predictive
              modeling.</strong>,
          </p>

          <p class="project-tech">Technology: Python, Pandas, Numpy</p>

          <div class="social-links">
            <a href="https://github.com/Glaramos" target="_blank"><i class="bi bi-github"></i></a>
            <a href="https://www.kaggle.com/code/gladysangelicaramos/data-cleaning-preparation-supermarket-sales"
              target="_blank"><i class="bi bi-database"></i>kaggle</a>
          </div>


          <!-- Tarjeta 1 -->
          <!-- <div class="project-card"> -->

          <p></p>
          <hr>
          <p></p>

          <section id="eda-Supermarket Sales Analysis">
            <h3>Supermarket Sales Analysis – Exploratory Data Analysis (EDA) with Python</h3>
            <p>This dataset contains detailed supermarket sales information, including product categories, quantities
              sold,
              payment methods, and daily transaction totals.
              The purpose of this notebook is to perform a complete Exploratory Data Analysis (EDA) to uncover
              patterns,
              identify high-performing products, and understand customer behavior.
              Workflow</p>

            <!-- Data Analysis Summary -->
            <ol>
              <li>
                <strong>Data Cleaning and Preparation</strong>
                <ul>
                  <li>Conversion of <em>fecha</em> to datetime</li>
                  <li>Grouping variables by category, product, and payment method</li>
                  <li>Calculation of totals, quantities, and averages</li>
                </ul>
              </li>
              <li>
                <strong>Visual Analysis</strong>
                <ul>
                  <li>Barplot: Total sales by product category</li>
                  <li>Barplot: Most sold products by quantity</li>
                  <li>Barplot: Total revenue by payment method</li>
                  <li>Lineplot: Daily sales trend</li>
                  <li>Barplot: Top-5 products by average ticket (mean sale price)</li>
                </ul>
              </li>
            </ol>

            <h2>Key Insights</h2>
            <ul>
              <li>Categories and products with the highest contribution to total revenue</li>
              <li>Items with greatest sales volume</li>
              <li>Preferred payment methods among customers</li>
              <li>Sales fluctuations over time</li>
              <li>High-value products with the highest average transaction amount</li>
            </ul>

            <p>All visualizations were created using <strong>Seaborn</strong> and <strong>Matplotlib</strong>, and the
              analysis was performed entirely in <strong>Python</strong>.</p>
            </ul>

            <p class="project-tech">Technology: Python, Seaborn, Pandas</p>

            <div class="social-links">
              <a href="https://github.com/Glaramos" target="_blank"><i class="bi bi-github"></i></a>
              <a href="https://www.kaggle.com/code/gladysangelicaramos/supermarket-sales-analysis" target="_blank"><i
                  class="bi bi-database"></i>kaggle</a>

            </div>
        </div>

      </div>


    </div>
  </div>

  </main>
  </div>


  <!-- Footer centrado con enlace de scroll -->
  <footer class="text-center py-3 mt-5">
    © 2025 Gladys Ramos — Portafolio ·
    <a href="#" id="scrollTopLink">Go up</a>
  </footer>
  </div>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
  <script src="assets/js/animation.js"></script>

  <!-- Scroll suave para "Volver arriba" -->
  <script>
    document
      .getElementById("scrollTopLink")
      .addEventListener("click", function (e) {
        e.preventDefault();
        window.scrollTo({ top: 0, behavior: "smooth" });
      });
  </script>
</body>

</html>